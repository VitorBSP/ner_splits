{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hartb/Estudos/mestrado/ner-splits/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ─────────────────────────────\n",
    "# 1. Standard library (já vem com o Python)\n",
    "# ─────────────────────────────\n",
    "import json\n",
    "from pathlib import Path\n",
    "import itertools  # se for gerar folds manualmente, por ex.\n",
    "import random  # sementes\n",
    "\n",
    "# ─────────────────────────────\n",
    "# 2. Terceiros – instalação via pip/venv\n",
    "# ─────────────────────────────\n",
    "# ↳ dados\n",
    "import pandas as pd  # leitura CSV\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "# ↳ modelagem e métricas\n",
    "import numpy as np\n",
    "# import torch\n",
    "# from transformers import (\n",
    "#     AutoTokenizer,\n",
    "#     AutoModelForTokenClassification,\n",
    "#     Trainer,\n",
    "#     TrainingArguments,\n",
    "# )\n",
    "# from seqeval.metrics import f1_score  # F1 para NER\n",
    "# from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# # (opcional) tracking e visualização\n",
    "# import mlflow  # ou tensorboard\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLEAN_PATH = Path(\"CleanCoNLL/data/cleanconll_annotations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_conll(path: Path) -> Dataset:\n",
    "    \"\"\"\n",
    "    Lê um arquivo CoNLL (4 colunas) e devolve Dataset {tokens, ner_tags}.\n",
    "    \"\"\"\n",
    "    sents, labels = [], []\n",
    "    toks, tags = [], []\n",
    "\n",
    "    with path.open(encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.rstrip(\"\\n\")\n",
    "\n",
    "            # 1) sentença terminou → salva e zera buffers\n",
    "            if not line:\n",
    "                if toks:  # evita linhas duplas\n",
    "                    sents.append(toks)\n",
    "                    labels.append(tags)\n",
    "                    toks, tags = [], []\n",
    "                continue\n",
    "\n",
    "            # 2) comentários do flair começam com '#'\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            # 3) CoNLL: token POS CHUNK NER\n",
    "            parts = line.split()\n",
    "            tok, ner = parts[0], parts[-1]  # pega 1ª e última coluna\n",
    "            toks.append(tok)\n",
    "            tags.append(ner)\n",
    "\n",
    "    # adiciona a última sentença se o arquivo não termina em blank line\n",
    "    if toks:\n",
    "        sents.append(toks)\n",
    "        labels.append(tags)\n",
    "\n",
    "    return Dataset.from_dict({\"tokens\": sents, \"ner_tags\": labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train = load_conll(CLEAN_PATH / \"cleanconll_annotations.train\")\n",
    "clean_dev = load_conll(CLEAN_PATH / \"cleanconll_annotations.dev\")  # opcional\n",
    "clean_test = load_conll(CLEAN_PATH / \"cleanconll_annotations.test\")  # opcional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 14903\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 3449\n",
       "})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['tokens', 'ner_tags'],\n",
       "    num_rows: 14903\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-splits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
