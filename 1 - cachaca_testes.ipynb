{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 1. Standard library (j√° vem com o Python)\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "import json\n",
    "from pathlib import Path\n",
    "import itertools  # se for gerar folds manualmente, por ex.\n",
    "import random  # sementes\n",
    "\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# 2. Terceiros ‚Äì instala√ß√£o via pip/venv\n",
    "# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
    "# ‚Ü≥ dados\n",
    "import pandas as pd  # leitura CSV\n",
    "from datasets import Dataset, DatasetDict, load_dataset\n",
    "\n",
    "# ‚Ü≥ modelagem e m√©tricas\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from transformers import (\n",
    "#     AutoTokenizer,\n",
    "#     AutoModelForTokenClassification,\n",
    "#     Trainer,\n",
    "#     TrainingArguments,\n",
    "# )\n",
    "# from seqeval.metrics import f1_score  # F1 para NER\n",
    "# from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "# # (opcional) tracking e visualiza√ß√£o\n",
    "# import mlflow  # ou tensorboard\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "from seqeval.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"data/cachacaNER.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(CSV_PATH)\n",
    "\n",
    "# a) identificar coluna de senten√ßa\n",
    "for cand in [\"sentence_id\", \"sentence\", \"sent_id\"]:\n",
    "    if cand in df.columns:\n",
    "        SENT_COL = cand\n",
    "        break\n",
    "else:\n",
    "    raise KeyError(\n",
    "        \"N√£o encontrei coluna de ID de senten√ßa. \"\n",
    "        f\"Colunas dispon√≠veis: {list(df.columns)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentence'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SENT_COL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_record(sent_id, group):\n",
    "    return {\n",
    "        f\"{SENT_COL}\": sent_id,  # üëà  agora fica dispon√≠vel\n",
    "        \"tokens\": group[\"token\"].tolist(),\n",
    "        \"ner_tags\": group[\"tag\"].tolist(),\n",
    "    }\n",
    "\n",
    "\n",
    "records_cachaca = [\n",
    "    sent_to_record(sent_id, grp) for sent_id, grp in df.groupby(SENT_COL, sort=False)\n",
    "]\n",
    "cachaca_full = Dataset.from_list(records_cachaca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'tokens', 'ner_tags'],\n",
       "    num_rows: 13628\n",
       "})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cachaca_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 130,\n",
       " 'tokens': ['NOME',\n",
       "  'DA',\n",
       "  'CACHA√áA',\n",
       "  ':',\n",
       "  'Porto',\n",
       "  'Estrela',\n",
       "  'Ouro',\n",
       "  '1',\n",
       "  'Litro'],\n",
       " 'ner_tags': ['O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'O',\n",
       "  'B-NOME_BEBIDA',\n",
       "  'I-NOME_BEBIDA',\n",
       "  'B-CLASSIFICACAO_BEBIDA',\n",
       "  'B-VOLUME',\n",
       "  'I-VOLUME']}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records_cachaca[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sentence'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SENT_COL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"trainingTest\" in df.columns:\n",
    "    train_idx = df[df[\"trainingTest\"] == \"training\"][SENT_COL].unique()\n",
    "    test_idx = df[df[\"trainingTest\"] == \"test\"][SENT_COL].unique()\n",
    "\n",
    "    cachaca_train = cachaca_full.select(\n",
    "        [i for i, rec in enumerate(records_cachaca) if rec[SENT_COL] in train_idx]\n",
    "    )\n",
    "    cachaca_test = cachaca_full.select(\n",
    "        [i for i, rec in enumerate(records_cachaca) if rec[SENT_COL] in test_idx]\n",
    "    )\n",
    "else:\n",
    "    # fallback: 80/20 aleat√≥rio\n",
    "    cachaca_train, cachaca_test = cachaca_full.train_test_split(\n",
    "        test_size=0.2, seed=42\n",
    "    ).values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'tokens', 'ner_tags'],\n",
       "    num_rows: 2847\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cachaca_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'tokens', 'ner_tags'],\n",
       "    num_rows: 9454\n",
       "})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cachaca_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_splits(\n",
    "    ds: Dataset, test_size=0.1, dev_size=0.1, seeds: List[int] = range(30)\n",
    ") -> List[DatasetDict]:\n",
    "    \"\"\"Retorna 30 divis√µes aleat√≥rias train/dev/test.\"\"\"\n",
    "    triples = []\n",
    "    for s in seeds:\n",
    "        train_tmp, test = ds.train_test_split(test_size=test_size, seed=s).values()\n",
    "        train, dev = train_tmp.train_test_split(\n",
    "            test_size=dev_size / (1 - test_size), seed=s\n",
    "        ).values()\n",
    "        triples.append(DatasetDict(train=train, dev=dev, test=test))\n",
    "    return triples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heur√≠stica 1 ‚ñ∏ comprimento\n",
    "lengths = np.array([len(t) for t in ds[\"tokens\"]])\n",
    "thr = np.percentile(lengths, 95)\n",
    "mask_test = lengths >= thr\n",
    "heur_len = DatasetDict(\n",
    "    train=ds.filter(~mask_test),\n",
    "    dev=ds.filter(mask_test),  # ‚Üê dev==test aqui para manter 80/20?\n",
    ")\n",
    "\n",
    "# Heur√≠stica 2 ‚ñ∏ palavra rara\n",
    "\n",
    "flat = [w.lower() for sent in ds[\"tokens\"] for w in sent]\n",
    "rare = {w for w, c in Counter(flat).items() if c <= 5}\n",
    "\n",
    "\n",
    "def has_rare(example):\n",
    "    return any(w.lower() in rare for w in example[\"tokens\"])\n",
    "\n",
    "\n",
    "heur_rare = DatasetDict(\n",
    "    train=ds.filter(lambda ex: not has_rare(ex)), dev=ds.filter(has_rare)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adversarial_split(ds: Dataset, k: int = int(0.1 * len(ds))) -> DatasetDict:\n",
    "    model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    embeds = model.encode(\n",
    "        [\" \".join(toks) for toks in ds[\"tokens\"]], show_progress_bar=True\n",
    "    )\n",
    "    tree = BallTree(embeds)\n",
    "    idx_train, idx_test = set(range(len(ds))), []\n",
    "\n",
    "    # pega o ponto mais central como semente do teste\n",
    "    idx = np.argmax(np.linalg.norm(embeds - embeds.mean(0), axis=1))\n",
    "    idx_train.remove(idx)\n",
    "    idx_test.append(idx)\n",
    "\n",
    "    while len(idx_test) < k:\n",
    "        # dist√¢ncia m√≠nima para qualquer j√° selecionado\n",
    "        dists, _ = tree.query(embeds[list(idx_train)], k=1, return_distance=True)\n",
    "        nxt = list(idx_train)[int(np.argmax(dists))]\n",
    "        idx_train.remove(nxt)\n",
    "        idx_test.append(nxt)\n",
    "\n",
    "    return DatasetDict(\n",
    "        train=ds.select(sorted(idx_train)),\n",
    "        dev=ds.select(sorted(idx_test)),  # dev==test de prop√≥sito (paper)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"neuralmind/bert-base-portuguese-cased\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    tok = tokenizer(batch[\"tokens\"], is_split_into_words=True, truncation=True)\n",
    "    # alinhar labels ‚Üí exerc√≠cio j√° feito antes\n",
    "    ...\n",
    "\n",
    "\n",
    "def compute_metrics(p):\n",
    "    logits, labels = p\n",
    "    preds = np.argmax(logits, -1)\n",
    "    # converter ids->tags, remover padding\n",
    "    ...\n",
    "    return {\"f1\": f1_score(true_tags, pred_tags)}\n",
    "\n",
    "\n",
    "def run_experiment(dsdict: DatasetDict, seed: int):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        MODEL, num_labels=NUM_LABELS\n",
    "    )\n",
    "\n",
    "    encoded = dsdict.map(tokenize, batched=True)\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"runs/seed{seed}\",\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        num_train_epochs=5,\n",
    "        seed=seed,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"f1\",\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=encoded[\"train\"],\n",
    "        eval_dataset=encoded.get(\"tune\", encoded[\"dev\"]),  # usa tune se existir\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer.evaluate(encoded[\"dev\"])[\"eval_f1\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 5.1  standard\n",
    "std_f1 = run_experiment(standard_split, seed=0)\n",
    "\n",
    "### 5.2  30 random seeds\n",
    "rand_scores = [\n",
    "    run_experiment(ds, s) for s, ds in enumerate(random_splits(cachaca_full))\n",
    "]\n",
    "\n",
    "### 5.3  heuristic + adversarial\n",
    "scores_heur_len = run_experiment(heur_len, 111)\n",
    "scores_heur_rare = run_experiment(heur_rare, 222)\n",
    "scores_advers = run_experiment(adversarial_split(cachaca_full), 333)\n",
    "\n",
    "### 5.4  tune-4-way\n",
    "tune_scores = run_experiment(tune_split(cachaca_full), 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as st\n",
    "\n",
    "print(f\"Standard: {std_f1:.3f}\")\n",
    "print(f\"Random   mean={st.mean(rand_scores):.3f}  sd={st.stdev(rand_scores):.3f}\")\n",
    "print(\n",
    "    f\"Heur-len {scores_heur_len:.3f} ‚Ä¢ Heur-rare {scores_heur_rare:.3f} ‚Ä¢ Advers {scores_advers:.3f}\"\n",
    ")\n",
    "print(f\"Tune-split dev F1 {tune_scores:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ner-splits",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
